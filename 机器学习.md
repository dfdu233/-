### 已知的情报

+ **题型** 名词解释+填空+问答











+ 计算题可能 **决策树** **贝叶斯** 二考一

+ 



### 涉及的知识

1. #### 判别模型和生成模型的区别（可能问答）

> **判别模型**：由数据直接学习决策Y=f(x)或条件概率分布P(Y|X)的模型即判别模型。
>
> + 即直接估计P(Y|X)
>
> **生成模型**：由训练数据学习联合概率分布P(X,Y),然后求得后验概率分布P(X|Y)
>
> + 具体来说
> + 即先估计P(X|Y),然后推导P(Y|X)
> + 朴素贝叶斯,HMM



+ 例子

![image-20240617212952511](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20240617212952511.png)



> 拉普拉斯平滑:分子加1，分母加上分子的种类数





2. #### 有监督 无监督学习各自的核心思想（可能问答）

**有监督**：对于输入X能预测Y  **无监督**：对于输入X能发现什么 与有监督相比，无监督学习差别重点在于输出y



3. #### SVM 凸优化 强队偶 弱对偶 对偶间隙（可能名词解释）

**凸优化**：
$$
domf是凸集，目标函数f(x)和不等式约束f_i(x)为凸函数，等式约束h_i(x)为仿射函数，\\凸优化目标在找到全局最优解x* \in domf,使得对任意x \in domf \  f(x*)\leq f(x)成立
$$


拉格朗日函数：为每个约束指定一个拉格朗日乘子，以乘子为系数将约束项加入目标函数

拉格朗日对偶函数（变量为拉格朗日乘子,即将原函数通过对x求导后消元）：对拉格朗日函数取下确界（乘子一定 x变的最小值）  

> 此对偶函数一定是凹函数，给出了原最优解的下界$$g(\lambda,v)=inf L(x,\lambda,v)\leq p^*(p^*是原问题最优解)$$

拉格朗日对偶问题：
$$
max[\lambda\geq0,v]g(\lambda,v),假设最优解为d^*
$$


**弱对偶**:
$$
d^*\leq p^* 给出原问题下界
$$
**强对偶**：
$$
d^*=p^* 如果(\leq 0 )约束均为凸函数一般强对偶成立 ，成立一般条件:Slater条件
$$
对偶间隙$$p^*-d^*$$

KKT条件：

> + 稳定性条件：拉格朗日函数偏导数为0
> + 约束条件成立：原始可行条件
>
> + $$\lambda \geq0$$：对偶可行性条件
> + $$\lambda_i f_i(x^*)=0$$：互补松弛条件



**SVM**

是一类以监督学习的方法进行二分类的分类模型

> 核心思想：寻找分类超平面使得样本点与超平面距离最大化
>
> 也被称为最大间隔分类器
>
> 支持向量：距离超平面最近的点

#### 支持向量机

算法思想：找到集合边缘若干数据，找出超平面，使得支持向量到该平凉距离最大
$$
超平面：w^Tx+b \\
约束 y_i(w^Tx_i+b)\geq 1\\
目标函数:\frac{2}{||w||}
$$




4. #### 常见决策树定义 相同点 不同点 核心原理，信息增益率

**原理**：

+ 归纳分类算法，通过学习训练集，挖掘规则
+ 基本算法是贪心 建树



**分类**：

![image-20240616210320258](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20240616210320258.png)

**ID3** **C4.5** **CART** 

> 相同点：树结构 支持分类 
>
> 不同点：信息增益率 CART使用基尼指数 CART支持回归
>
> + C4.5
>
>   + 核心：剪枝
>
>   + > 去掉一些分支降低过拟合（直接变成结论节点）
>
>   + 预剪枝和后剪枝
>
>   + 使用信息增益率



**（经验）熵**：$$H(x)=-\Sigma p_ilogp_i$$

**（经验）条件熵**：在X给定条件下Y的条件概率分布对应的熵对X的数学期望,代表已知X下Y的不确定性
$$
H(Y|X)=-\Sigma_{i=1}^{n} p_iH(Y|X=x_i) \ p_i表示x_i出现概率 H(Y|X=x_i)为缩小样本空间后的熵
$$
**信息增益**：得知特征X信息使得类Y的信息不确定减少的程度
$$
g(D,A)=H(D)-H(D|A) \\
信息增益越大，该特征分类能力越强
$$
例子：



5. #### SVM核函数定义 常见核函数（可能填空）

背景：无法找到超平面将两类样本分开

解决思路：对x作用非线性变换$$\phi$$，将x从原始空间映射到高维空间，使得$$\phi (x)$$可分

> $$
> 非线性优化函数：  \\
> 约束：
> $$
>
> 

+ $$直接定义\phi困难，引入K(x_i,x_j)=\phi (x_i) *\phi (x_j)$$



**常见核函数**：

+ 线性核函数 
+ 多项式核函数
+ 高斯核函数

![image-20240616221555697](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20240616221555697.png)







6. #### 数据增广，常见办法（可能填空 问答题）

**数据增广(Data augmentation)**：人为增加数据的多样性

> 颜色空间：亮度，灰度，对比度
>
> 几何空间：旋转，平移，缩放，弹性形变等



7. #### 神经网络 激活函数 常见类型

**激活函数分类**：

+ sigmoid $$f(x)=\frac{1}{1+e^{-x}}$$
+ tanh$$f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$
+ relu$$f(x)=max(0,x)$$

![image-20240617152445557](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20240617152445557.png)



8. #### 全连接，CNN，（卷积 池化 原理） （可能考名词解释）

**全连接**：

> + 两层之间的神经元均通过权值相连 
> + 同层的神经元间无连接(RNN同层神经元有连接)



**CNN**：

**卷积**：

> + 卷积核w沿着输入a的宽和高方向滑动
> + 输出z由w和a对位元素相乘得到
> + **等价于全连接中的局部连接 每次运算共享卷积核(与前面知识联系的角度)**
>
> 作用：提取特征 如垂直边缘与水平边缘

**池化**：

> **本质是下采样**
>
> 目的：在不引入可学习参数w前提下，高效的对输入下采样，降低输入的维度



9. #### 随机梯度下降流程

> 反向传播：

![image-20240617193810043](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20240617193810043.png)











10. #### 数据归一化方法与原理

> 

### 零碎复习

1. ##### 绪论

![image-20240616201145653](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20240616201145653.png)



+ 监督学习包含 **分类**（预测离散输出）  **回归**（预测连续输出）
+ 无监督学习包括 **聚类** **降维**
+ 强化学习：解决智能体与环境交互通过学习策略实现回报最大化
+ 库包括numpy scipy pandas matplotlib（绘图）
+ 流程：数据收集 数据清洗 特征工程 数据建模 

2. ##### 分类数据处理+评价指标



指标：ROC曲线（横轴FPR纵轴TPR）

> 
> $$
> FPR=\frac{FP}{FP+TN} 假阳率 错误的判断阳性占
> $$
> FPR=



3. ##### 贝叶斯方法

> + 先验概率：记作P(Y)，是指在观测数据前得到的概率与数据无关（比如某种疾病发病率）
> + 后验概率：记作P(Y|X)，根据已经发生的事件分析的条件，反应得到X后，Y发生的概率
> + 掌握根据鲜艳求后验



**贝叶斯公式**:
$$
P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}=\frac{P(X|Y)P(Y)}{P(X|Y)P(Y)+P(X|\hat{Y})P(\hat{Y})}
$$



**例子**：事件X:某人患有疾病 概率 P(X)=0.1%,事件Y:在已知患病情况下，99%可见查出阳性P(Y|X)=99%(换个说话，检测准确率是99%)

问：检出阳性的情况下患病的可能性P(X|Y)
$$
P(X|Y)=\frac{P(Y|X)P(X)}{P(Y|\hat{X})P(\hat{X})+P(Y|X)P(X)}
$$


